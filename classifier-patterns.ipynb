{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier patterns\n",
    "\n",
    "Matthew Stone, CS 533, Spring 2017, to accompany first homework.\n",
    "\n",
    "This notebook is designed to get you started thinking and coding for the first homework.  It includes illustrations of how to use the software that I have provided for the assignment.  It includes some cool bits of python that you might not have thought of (or known to google for) that make many of the things you'll have to do very easy.  And it includes [some design patterns][1] for exploring representation changes in classification problems that you can work with as you push your ideas forward.\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Software_design_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import vocabulary\n",
    "import newsreader\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Pattern 0: Keep the things you have to customize separate\n",
    "\n",
    "It's always easy to get carried away getting settled into a particular setup and working on a particular experiment.  But the first step in making your work general is [DRY: don't repeat yourself][1].  That means setting up variables once and for all for the things you're going to have to get right to work with particular data in a particular environment.  Let's get organized.\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Don't_repeat_yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_file, vocab_file_type = \"vocab.pkl\", \"pickle\"\n",
    "\n",
    "embedding_file, embedding_dimensions, embedding_cache = \\\n",
    "    \"glove.6B.50d.txt\", 50, \"embedding.npz\"\n",
    "\n",
    "train_dir, test_dir = \\\n",
    "    \"20news-bydate/20news-bydate-train/\", \"20news-bydate/20news-bydate-test/\"\n",
    "\n",
    "AUTOS, MOTORCYCLES, RELIGION = \\\n",
    "    \"rec.autos\", \"rec.motorcycles\", \"talk.religion.misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Pattern 1: Save your work\n",
    "\n",
    "You won't always be able to keep your notebook running indefinitely.  Some of the computations you will want to do in putting together the assignment are big.  So consider using the [python pickle serialization framework][1] and [loading and saving functionality from numpy][2] in cases like this to let you pick up right where you left off.\n",
    "\n",
    "[1]:http://www.ibm.com/developerworks/library/l-pypers/index.html\n",
    "[2]:https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "made_vocabulary = True\n",
    "if made_vocabulary :\n",
    "    v = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
    "else: \n",
    "    v = vocabulary.Vocabulary.from_iterable(newsreader.all_20newsgroup_tokens(\"20news-bydate\"),\n",
    "                                            file_type=vocab_file_type)\n",
    "    v.save(vocab_file)\n",
    "v.stop_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "made_embedding = True\n",
    "if made_embedding :\n",
    "    e = newsreader.load_sparse_csr(embedding_cache)\n",
    "else: \n",
    "    e = newsreader.build_sparse_embedding(v, embedding_file, embedding_dimensions)\n",
    "    newsreader.save_sparse_csr(embedding_cache, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Finding features\n",
    "\n",
    "You're going to want to use the data manager class, which has placeholders that say how you might build a feature dictionary by using the training data and then how you will scan files to create feature matrices from file tokens.  Here are default functions that carry out those operations in the simplest way possible.  **Remember: simple is good.** Being fancy is just as likely to get you into trouble as it is to help you.  If you have these functions defined, you can make a data manager with a call like this (where `v` is your vocabulary)\n",
    "\n",
    "```python\n",
    "my_data = newsreader.DataManager(class1_training_directory,\n",
    "                                 class2_training_directory,\n",
    "                                 class1_test_directory,\n",
    "                                 class2_test_directory,\n",
    "                                 use_default_features(v), \n",
    "                                 count_features)\n",
    "```\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_default_features(vocab) :\n",
    "    return lambda data: v\n",
    "\n",
    "def count_features(features, gen_tokens) :\n",
    "    for t in gen_tokens :\n",
    "        r = features.add(t)\n",
    "        if r :\n",
    "            yield r    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Pattern 2: Higher-order functions for one-off abstraction\n",
    "\n",
    "We're going to be using `scipy` sparse matrices to handle language data (as we must), but unfortunately it turns out that a lot of the nice operations that you can do on `numpy` matrices don't work on sparse matrices, because they wouldn't preserve the sparsity in the general case.  One of the things that turns out to be impossible is to \"forget\" the  values in a matrix and replace all the nonzero values in a matrix with a sensible value like 1.  Bottom line: if we want to have yes/no features in our data matrices, rather than count features, we're going to have to remove redundancies right away when we scan the file.\n",
    "\n",
    "Now, if you think for a second, you'll realize that this effect doesn't have anything to do with what features we're looking for or how we're looking for them.  Features are always integers, and the way we merge redundancies is to accumulate the features that we've seen so far into a set and, once we're done scanning a file, we then emit the features we observed.\n",
    "\n",
    "Lots of languages (but maybe not the ones that you're used to) let you just parameterize this operation by the basic `feature_counter` operation, and create abstract, general code without the nuisance of making objects, classes, data types or other heavy-duty discipline.  Here's how you do it in python.  As it happens, I wanted to experiment with binary features in exploring the design space for text classifiers today, so this is what I will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_binary_features(feature_counter) :\n",
    "    def collect_features(features, gen_tokens) :\n",
    "        seen = set()\n",
    "        for f in feature_counter(features, gen_tokens) :\n",
    "            seen.add(f)\n",
    "        for f in seen :\n",
    "            yield f\n",
    "    return collect_features\n",
    "\n",
    "autos_vs_religion = newsreader.DataManager(train_dir + AUTOS,\n",
    "                                           train_dir + RELIGION,\n",
    "                                           test_dir + AUTOS,\n",
    "                                           test_dir + RELIGION,\n",
    "                                           use_default_features(v),\n",
    "                                           make_binary_features(count_features))\n",
    "autos_vs_religion.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Pattern 3: Bundle your data and provide consistent views\n",
    "\n",
    "From data management, we've learned that analysts are often interested in [views][1], which are systematically related to underlying data but record the results of inferences and other computations which are precomputed so that you can have fast access to them.\n",
    "\n",
    "This is going to be important as you explore the best ways  to represent the underlying text, and design experiments that are going to compare these representations to one another.  \n",
    "\n",
    "This is the kind of thing that it's nice to formalize with a python class.  The `Experiment` class below sets up the problem of learning a classifier from a data manager, arranges the data acquisition and classifier training, and carries out a preliminary validation.  Most importantly, it provides a general method for taking your data, rerepresenting it in a consistent way, and setting up another experiment as a result.\n",
    "\n",
    "### Design Pattern 4: But keep the test data sacred\n",
    "\n",
    "**You don't want to make it easy to peek at the test data, which is special and limited.** So even though this class keeps the structure in place so that you manage a consistent representation of the test data, you have to add code to evaluate on the test data yourself.\n",
    "\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/View_(SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experiment(object) :\n",
    "    '''Organize the process of getting data, building a classifier,\n",
    "    and exploring new representations'''\n",
    "    \n",
    "    def __init__(self, data, classifier) :\n",
    "        'set up the problem of learning a classifier from a data manager'\n",
    "        self.data = data\n",
    "        self.classifier = classifier\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self) :\n",
    "        'materialize the training data, dev data and test data as matrices'\n",
    "        if not self.initialized :\n",
    "            self.train_X, self.train_y = self.data.training_data()\n",
    "            self.dev_X, self.dev_y = self.data.dev_data()\n",
    "            self.test_X, self.test_y = self.data.test_data()\n",
    "            self.initialized = True\n",
    "        \n",
    "    def fit_and_validate(self) :\n",
    "        'train the classifier and assess predictions on dev data'\n",
    "        if not self.initialized :\n",
    "            self.initialize()\n",
    "        self.classifier.fit(self.train_X, self.train_y)\n",
    "        self.dev_predictions = self.classifier.predict(self.dev_X)\n",
    "        self.accuracy = sklearn.metrics.accuracy_score(self.dev_y, self.dev_predictions)\n",
    "                \n",
    "    @classmethod\n",
    "    def transform(cls, expt, operation, classifier) :\n",
    "        'use operation to transform the data from expt and set up new classifier'\n",
    "        if not expt.initialized :\n",
    "            expt.initialize()\n",
    "        result = cls(expt.data, classifier)\n",
    "        result.train_X, result.train_y = operation(expt.train_X, expt.train_y)\n",
    "        result.dev_X, result.dev_y = operation(expt.dev_X, expt.dev_y)\n",
    "        result.test_X, result.test_y = operation(expt.test_X, expt.test_y)\n",
    "        result.initialized = True\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Setting up an experiment\n",
    "\n",
    "Pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93434343434343436"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt1 = Experiment(autos_vs_religion,\n",
    "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt1.initialize()\n",
    "expt1.fit_and_validate()\n",
    "expt1.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Getting similarity data\n",
    "\n",
    "One thing that you have to do as part of the assignment is to come up with a way of measuring how different the documents in different classes are. Getting the data is surprisingly easy with a little savvy with `numpy`, `scipy` and `scikit-learn`.  Let's look at the ingredients.\n",
    "\n",
    "* [scikit-learn's pairwise distances operation.][1]  This is pretty amazing and necessary in that it works on sparse matrices and gives you pairwise distance measures across a set of items simultaneously.  Cosine distance is one of the options.  Passing in one matrix gives you the distances between all the rows in the matrix; passing in two matrices gives you the distances between each row in one and each row in the other.\n",
    "* [Boolean indexing and row slices][2].  It's absurdly easy to get all the rows of the training data from one class or the other class with this elegant (and surprisingly readable) program construct.  This happens to work nicely here because we've stored the data as a column-sparse-row matrix which allows fast and flexible row slices.\n",
    "\n",
    "Bottom line: this teeny function gives you a matrix of the distances between items of differing categories, and two matrices of distances between items of the same category, for your further exploration.\n",
    "\n",
    "[1]:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "[2]:https://docs.scipy.org/doc/numpy-1.10.1/user/basics.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difficulty(expt) :\n",
    "    # this was too long to write out over and over\n",
    "    # plus you want to be sure to have the same distance across the board\n",
    "    # DRY!\n",
    "    d = lambda *args: sklearn.metrics.pairwise.pairwise_distances(*args, metric='cosine')\n",
    "    \n",
    "    Xyes = expt.train_X[expt.train_y ==1, :]\n",
    "    Xno = expt.train_X[expt.train_y != 1, :] \n",
    "    return d(Xyes, Xno), d(Xyes), d(Xno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Pattern 5: Test\n",
    "\n",
    "If you want to understand similarity, why not play with what you can compute from the difficulty matrices AUTOS-RELIGION and AUTOS-MOTORCYCLES.  Is the answer what you expect... once you measure distance the right way?\n",
    "\n",
    "### Design Pattern 6: But remember that your tests ruin the data\n",
    "\n",
    "Just don't include the AUTOS-RELIGION and AUTOS-MOTORCYCLES comparisons in the final evaluation you put together.  If you've already peeked, and in fact tuned what you've done, your experiment won't tell you anything new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93939393939393945"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos_vs_bikes = newsreader.DataManager(train_dir + AUTOS,\n",
    "                                        train_dir + MOTORCYCLES,\n",
    "                                        test_dir+ AUTOS,\n",
    "                                        test_dir + MOTORCYCLES,\n",
    "                                        use_default_features(v),\n",
    "                                        make_binary_features(count_features))\n",
    "autos_vs_bikes.initialize()\n",
    "\n",
    "expt1b = Experiment(autos_vs_bikes,\n",
    "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt1b.initialize()\n",
    "expt1b.fit_and_validate()\n",
    "expt1b.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Making feature values meaningful.\n",
    "\n",
    "In preparing this assignment, I read [an ACL 2012 paper by Sida Wang and Chris Manning][2] about text classification.  They suggested weighting word features not based on counts, or 1-0, or based on TF-IDF, but instead based on the evidence that the features provide for the eventual class.  This is a cool and influential idea.  All you need to implement it is to work with binary features, compute a score for each feature, and then reweight by the scores.  It's crazy simple code.\n",
    "* Use boolean indexing and slices to get the data\n",
    "* Sparse matrices have built-in methods for counting nonzero elements\n",
    "* Numpy lets you compute the relevant operations elementwise over the matrix.\n",
    "\n",
    "This is not just a one-off pattern for this particular research result.  For example, if you don't bother to separate the positive and negative examples, and add a call to [np.reciprocal][1], you'll basically turn this into TFIDF weighting.\n",
    "\n",
    "[1]:https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.reciprocal.html\n",
    "[2]:http://aclweb.org/anthology/P/P12/P12-2018.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reweight_features(expt) :\n",
    "    Xyes = expt.train_X[expt.train_y ==1, :]\n",
    "    Xno = expt.train_X[expt.train_y != 1, :] \n",
    "    yesrates = np.log((Xyes.getnnz(axis=0) + 1.) / Xyes.shape[1])\n",
    "    norates = np.log((Xno.getnnz(axis=0) + 1.) / Xno.shape[1])\n",
    "    W = scipy.sparse.diags(yesrates - norates, 0)\n",
    "    return lambda X, y: (X.dot(W), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infrastructure that we have in place already to work with data makes this super-easy to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96464646464646464"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt2 = Experiment.transform(expt1,\n",
    "                             reweight_features(expt1),\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt2.fit_and_validate()\n",
    "\n",
    "expt2.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Drawing on additional resources\n",
    "\n",
    "One of the important ways to improve results on NLP problems is to get access to additional resources.  You always have a limited amount of training data for the problem you're interested in, which will include instances with properties that you have seen rarely if at all.  If you have some general sense of how words behave, from text corpora or dictionaries, you can use that information to generalize from words that you have seen to words that you haven't.\n",
    "\n",
    "Word embeddings, of course, are a prototypical example of this.  To use them, you just add features to your data points computed from the word embeddings.  The sparse matrix `stack` operations make it easy to construct the matrices you need. (Note: if you have extra features that aren't in the word embedding, e.g., bigrams, then you need to account for that in applying the generic word embeddings; conversely, you probably want to add the word embedding features to your problem, since you may actually have information about particular words that gives special but reliable information about your classification problem that you do not want to discard.)  With these hints, you can probably understand the code for adding embedding information to your classification problem.\n",
    "\n",
    "Another thing you could try along these lines would be to build an embedding matrix by applying SVD to the training data, as in the latent semantic analysis exercise we did two weeks ago.\n",
    "\n",
    "```python\n",
    "_, _, wrt = scipy.sparse.linalg.svds(expt.X_train, k=r)\n",
    "W = np.transpose(wrt))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96969696969696972"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stack_embeddings(embeddings) :\n",
    "    def operation(X, y) :\n",
    "        extra_features = X.shape[1] - embeddings.shape[0]\n",
    "        if extra_features > 0 :\n",
    "            Z = scipy.sparse.csr_matrix((extra_features, embeddings.shape[1]))\n",
    "            W = scipy.sparse.vstack([embeddings, Z])\n",
    "        else: \n",
    "            W = embeddings\n",
    "        return scipy.sparse.hstack([X, X.dot(W)]), y\n",
    "    return operation\n",
    "\n",
    "expt3 = Experiment.transform(expt2,\n",
    "                             stack_embeddings(e),\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt3.fit_and_validate()\n",
    "expt3.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Fancy feature definitions\n",
    "\n",
    "The place that you get a payoff from a good design is when you're able to easily try something out that you didn't anticipate when you started writing the code.  Invariably, that's a bit lucky when it works out, and anything that you start pushing too hard in a direction that it wasn't designed for becomes more and more cumbersome until you can't stand it.  But we're not there yet in this assignment.\n",
    "\n",
    "Last time we looked at [nltk's resources for analyzing collocations][1], because, as we saw, it's an interesting unsupervised technique for characterizing word senses and dealing with ambiguous words.  The architecture that we have for identifying features in the training data makes it possible to preprocess the training data to look for meaningful bigram features.  (The strategy I've used here is not exactly the one that [Wang and Manning][2] recommend by the way!)  Python generator functions make it straightforward to report instances of these features in the course of scanning through a file.  \n",
    "\n",
    "So it's totally routine to try this out by adapting the work we've done so far.  Who knows what else we could handle this way!\n",
    "\n",
    "[1]:http://www.nltk.org/howto/collocations.html\n",
    "[2]:http://aclweb.org/anthology/P/P12/P12-2018.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def use_bigram_features(data) :\n",
    "    f = vocabulary.Vocabulary.load(\"vocab.pkl\", file_type=\"pickle\")\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    word_fd = nltk.FreqDist(data.all_train_tokens())\n",
    "    bigram_fd = nltk.FreqDist(nltk.bigrams(data.all_train_tokens()))\n",
    "    finder = nltk.collocations.BigramCollocationFinder(word_fd, bigram_fd)\n",
    "    finder.apply_freq_filter(5)\n",
    "    collocations = filter(lambda (g,i): i > 0, finder.score_ngrams(bigram_measures.pmi))\n",
    "    for (w1, w2), _ in collocations:\n",
    "        f.add(w1 + \" \" + w2)\n",
    "    f.stop_growth()\n",
    "    return f\n",
    "\n",
    "def count_bigram_features(features, gen_tokens) :\n",
    "    prev = None\n",
    "    for t in gen_tokens :\n",
    "        r = features.add(t)\n",
    "        if r :\n",
    "            yield r\n",
    "            if prev :\n",
    "                r = features.add(prev + \" \" + t)\n",
    "                if r : \n",
    "                    yield r\n",
    "            prev = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autos_vs_religion_b = newsreader.DataManager(train_dir + AUTOS,\n",
    "                                             train_dir + RELIGION,\n",
    "                                             test_dir + AUTOS,\n",
    "                                             test_dir + RELIGION,\n",
    "                                             use_bigram_features,\n",
    "                                             make_binary_features(count_bigram_features))\n",
    "autos_vs_religion_b.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92929292929292928"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt4 = Experiment(autos_vs_religion_b,\n",
    "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt4.initialize()\n",
    "expt4.fit_and_validate()\n",
    "\n",
    "expt4.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94444444444444442"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt5 = Experiment.transform(expt4,\n",
    "                             stack_embeddings(e),\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=5))\n",
    "expt5.fit_and_validate()\n",
    "\n",
    "expt5.accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deepnl]",
   "language": "python",
   "name": "conda-env-deepnl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
