{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import string\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_131/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('articles_with_texts_final.json') as data_file:\n",
    "    articles = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_keys = articles[u'data'].keys()\n",
    "vectorizer = \\\n",
    "sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, \n",
    "                                                max_df=0.5, \n",
    "                                                stop_words='english')\n",
    "_ = vectorizer.fit(articles[u'data'][k][u'text'] for k in article_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T_keys = [k for k in article_keys if (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]\n",
    "F_keys = [k for k in article_keys if not (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-211-987cb9d89d99>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-211-987cb9d89d99>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    return [0.0 0.0 0.0]\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#def feature1(c_articles) : #c_articles is [articles[u'data'][k] for k in keys]\n",
    "\n",
    "#author_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "#_ = author_vectorizer.fit(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys])\n",
    "\n",
    "def author(cas):\n",
    "    return author_vectorizer.transform(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in cas)\n",
    "    \n",
    "# truth with noise\n",
    "def truth(cas):\n",
    "    if len(cas) == 4000 :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[:4000]], (-1, 1))\n",
    "    else :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[4000:]], (-1, 1))\n",
    "\n",
    "st = StanfordNERTagger('C:\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz', 'C:\\stanford-ner\\stanford-ner.jar', encoding='utf-8')\n",
    "    \n",
    "def NE_count(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    t = st.tag(tokenized_text)\n",
    "    n_persons = 0.0\n",
    "    n_org = 0.0\n",
    "    n_loc = 0.0\n",
    "    if len(t) == 0 :\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    (a, b) = t[0]\n",
    "    if b == u'PERSON' :\n",
    "        n_persons += 1\n",
    "    elif b == u'LOCATION' :\n",
    "        n_loc += 1\n",
    "    elif b == u'ORGANIZATION' :\n",
    "        n_org += 1\n",
    "    prev = b\n",
    "    for (a, b) in t[1:] :\n",
    "        if b != prev :\n",
    "            prev = b\n",
    "            if b == u'PERSON' :\n",
    "                n_persons += 1\n",
    "            elif b == u'LOCATION' :\n",
    "                n_loc += 1\n",
    "            elif b == u'ORGANIZATION' :\n",
    "                n_org += 1\n",
    "    return numpy.matrix([n_persons/len(tokenized_text), n_loc/len(tokenized_text), n_org/len(tokenized_text)])\n",
    "\n",
    "def named_entity(cas):\n",
    "    return numpy.matrix([NE_count(ca[u'text']) for ca in cas])\n",
    "\n",
    "features = [named_entity]\n",
    "\n",
    "\n",
    "\n",
    "def getX(key_list):\n",
    "    x = vectorizer.transform(articles[u'data'][k][u'text'] for k in key_list)\n",
    "    c_articles = [articles[u'data'][k] for k in key_list]\n",
    "    for f in features :\n",
    "        x_new = f(c_articles)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys[:4000]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = getX(article_keys[:4000])\n",
    "y_train = [k in T_keys for k in article_keys[:4000]]\n",
    "X_dev = getX(article_keys[4000:])\n",
    "y_dev = [k in T_keys for k in article_keys[4000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = \\\n",
    "sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                   penalty=\"elasticnet\",\n",
    "                                   n_iter=5)\n",
    "\n",
    "_ = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_dev)\n",
    "print sklearn.metrics.accuracy_score(y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word = vectorizer.get_feature_names()\n",
    "best_elts = numpy.argpartition(classifier.coef_, classifier.coef_.size - 20)[0][-20:]\n",
    "best_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in best_elts]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_elts = numpy.argpartition(classifier.coef_, 19)[0][:20]\n",
    "worst_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in worst_elts]\n",
    "worst_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author([articles[u'data'][k] for k in T_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author([articles[u'data'][k] for k in F_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NE_train = X_train.tocsc()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NE_dev = X_dev.tocsc()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NE_count(articles[u'data'][T_keys[0]][u'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NE_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"NE_count_dev.txt\", 'w') as f :\n",
    "    for i in NE_dev:\n",
    "        f.write(str(i))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(articles[u'data'][article_keys[0]][u'text'])\n",
    "t = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
