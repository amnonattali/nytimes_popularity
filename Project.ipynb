{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import string\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_131/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "st = StanfordNERTagger('C:\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz', 'C:\\stanford-ner\\stanford-ner.jar', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('articles_with_texts_final.json') as data_file:\n",
    "    articles = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_keys = articles[u'data'].keys()\n",
    "vectorizer = \\\n",
    "sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, \n",
    "                                                max_df=0.5, \n",
    "                                                stop_words='english')\n",
    "_ = vectorizer.fit(articles[u'data'][k][u'text'] for k in article_keys[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T_keys = [k for k in article_keys if (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]\n",
    "F_keys = [k for k in article_keys if not (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def feature1(c_articles) : #c_articles is [articles[u'data'][k] for k in keys]\n",
    "\n",
    "#author_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "#_ = author_vectorizer.fit(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys])\n",
    "\n",
    "def author(cas):\n",
    "    return author_vectorizer.transform(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in cas)\n",
    "    \n",
    "# truth with noise\n",
    "def truth(cas):\n",
    "    if len(cas) == 4000 :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[:4000]], (-1, 1))\n",
    "    else :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[4000:]], (-1, 1))\n",
    "\n",
    "NE_count_file = 'NE_counts.txt'\n",
    "    \n",
    "def NE_count(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    t = st.tag(tokenized_text)\n",
    "    n_persons = 0.0\n",
    "    n_org = 0.0\n",
    "    n_loc = 0.0\n",
    "    if len(t) == 0 :\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    (a, b) = t[0]\n",
    "    if b == u'PERSON' :\n",
    "        n_persons += 1\n",
    "    elif b == u'LOCATION' :\n",
    "        n_loc += 1\n",
    "    elif b == u'ORGANIZATION' :\n",
    "        n_org += 1\n",
    "    prev = b\n",
    "    for (a, b) in t[1:] :\n",
    "        if b != prev :\n",
    "            prev = b\n",
    "            if b == u'PERSON' :\n",
    "                n_persons += 1\n",
    "            elif b == u'LOCATION' :\n",
    "                n_loc += 1\n",
    "            elif b == u'ORGANIZATION' :\n",
    "                n_org += 1\n",
    "    return [n_persons/len(tokenized_text), n_loc/len(tokenized_text), n_org/len(tokenized_text)]\n",
    "\n",
    "def named_entity(keys):\n",
    "    NE_count_dict = {}\n",
    "    if NE_count_file != None :\n",
    "        with open(NE_count_file, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                NE_count_dict[l[0].encode()] = [float(l[1]), float(l[2]), float(l[3])]\n",
    "        return numpy.matrix([NE_count_dict[key] for key in keys])\n",
    "    else:\n",
    "        return numpy.matrix([NE_count(articles[u'data'][key][u'text']) for key in keys])\n",
    "\n",
    "features = []\n",
    "features_k = [named_entity]\n",
    "\n",
    "\n",
    "def getX(key_list):\n",
    "    x = vectorizer.transform(articles[u'data'][k][u'text'] for k in key_list)\n",
    "    c_articles = [articles[u'data'][k] for k in key_list]\n",
    "    for f in features :\n",
    "        x_new = f(c_articles)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    for f in features_k :\n",
    "        x_new = f(key_list)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys[:4000]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = getX(article_keys[:4000])\n",
    "y_train = [k in T_keys for k in article_keys[:4000]]\n",
    "X_dev = getX(article_keys[4000:])\n",
    "y_dev = [k in T_keys for k in article_keys[4000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = \\\n",
    "sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                   penalty=\"elasticnet\",\n",
    "                                   n_iter=5)\n",
    "\n",
    "_ = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_dev)\n",
    "print sklearn.metrics.accuracy_score(y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word = vectorizer.get_feature_names()\n",
    "best_elts = numpy.argpartition(classifier.coef_, classifier.coef_.size - 20)[0][-20:]\n",
    "best_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in best_elts]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_elts = numpy.argpartition(classifier.coef_, 19)[0][:20]\n",
    "worst_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in worst_elts]\n",
    "worst_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('NE_counts.txt', 'w') as f:\n",
    "#    for i in range(4000):\n",
    "#        f.write(article_keys[i] + ' ' + str(NE_train[i][0]) + ' ' + str(NE_train[i][1]) + ' ' + str(NE_train[i][2]) + '\\n')\n",
    "#    for i in range(len(NE_dev)):\n",
    "#        f.write(article_keys[4000+i] + ' ' + str(NE_dev[i][0]) + ' ' + str(NE_dev[i][1]) + ' ' + str(NE_dev[i][2]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getX(article_keys[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
