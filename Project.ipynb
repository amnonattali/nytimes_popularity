{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import string\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_131/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('articles_with_texts_final.json') as data_file:\n",
    "    articles = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_keys = articles[u'data'].keys()\n",
    "vectorizer = \\\n",
    "sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, \n",
    "                                                max_df=0.5, \n",
    "                                                stop_words='english')\n",
    "_ = vectorizer.fit(articles[u'data'][k][u'text'] for k in article_keys[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T_keys = [k for k in article_keys if (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]\n",
    "F_keys = [k for k in article_keys if not (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def feature1(c_articles) : #c_articles is [articles[u'data'][k] for k in keys]\n",
    "\n",
    "#author_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "#_ = author_vectorizer.fit(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys])\n",
    "\n",
    "def author(cas):\n",
    "    return author_vectorizer.transform(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in cas)\n",
    "    \n",
    "# truth with noise\n",
    "def truth(cas):\n",
    "    if len(cas) == 4000 :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[:4000]], (-1, 1))\n",
    "    else :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[4000:]], (-1, 1))\n",
    "\n",
    "st = StanfordNERTagger('C:\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz', 'C:\\stanford-ner\\stanford-ner.jar', encoding='utf-8')\n",
    "    \n",
    "NE_count_file = 'NE_counts.txt'\n",
    "    \n",
    "def NE_count(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    t = st.tag(tokenized_text)\n",
    "    n_persons = 0.0\n",
    "    n_org = 0.0\n",
    "    n_loc = 0.0\n",
    "    if len(t) == 0 :\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    (a, b) = t[0]\n",
    "    if b == u'PERSON' :\n",
    "        n_persons += 1\n",
    "    elif b == u'LOCATION' :\n",
    "        n_loc += 1\n",
    "    elif b == u'ORGANIZATION' :\n",
    "        n_org += 1\n",
    "    prev = b\n",
    "    for (a, b) in t[1:] :\n",
    "        if b != prev :\n",
    "            prev = b\n",
    "            if b == u'PERSON' :\n",
    "                n_persons += 1\n",
    "            elif b == u'LOCATION' :\n",
    "                n_loc += 1\n",
    "            elif b == u'ORGANIZATION' :\n",
    "                n_org += 1\n",
    "    return [n_persons/len(tokenized_text), n_loc/len(tokenized_text), n_org/len(tokenized_text)]\n",
    "\n",
    "def named_entity(keys):\n",
    "    NE_count_dict = {}\n",
    "    if NE_count_file != None :\n",
    "        with open(NE_count_file, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                NE_count_dict[encode(l[0])] = [float(l[1]), float(l[2]), float(l[3])]\n",
    "        return numpy.matrix([NE_count_dict[key] for key in keys])\n",
    "    else:\n",
    "        return numpy.matrix([NE_count(articles[u'data'][key][u'text']) for key in keys])\n",
    "\n",
    "features = []\n",
    "features_k = [named_entity]\n",
    "\n",
    "\n",
    "def getX(key_list):\n",
    "    x = vectorizer.transform(articles[u'data'][k][u'text'] for k in key_list)\n",
    "    c_articles = [articles[u'data'][k] for k in key_list]\n",
    "    for f in features :\n",
    "        x_new = f(c_articles)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    for f in features_k :\n",
    "        x_new = f(key_list)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys[:4000]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = getX(article_keys[:4000])\n",
    "y_train = [k in T_keys for k in article_keys[:4000]]\n",
    "X_dev = getX(article_keys[4000:])\n",
    "y_dev = [k in T_keys for k in article_keys[4000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = \\\n",
    "sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                   penalty=\"elasticnet\",\n",
    "                                   n_iter=5)\n",
    "\n",
    "_ = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_dev)\n",
    "print sklearn.metrics.accuracy_score(y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word = vectorizer.get_feature_names()\n",
    "best_elts = numpy.argpartition(classifier.coef_, classifier.coef_.size - 20)[0][-20:]\n",
    "best_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in best_elts]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_elts = numpy.argpartition(classifier.coef_, 19)[0][:20]\n",
    "worst_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in worst_elts]\n",
    "worst_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1980)\t0.0991455170763\n",
      "  (0, 2189)\t0.162397645228\n",
      "  (0, 2197)\t0.0516169289831\n",
      "  (0, 2314)\t0.0491153881479\n",
      "  (0, 2729)\t0.103925922652\n",
      "  (0, 3105)\t0.0307984579772\n",
      "  (0, 3283)\t0.067482745956\n",
      "  (0, 3361)\t0.0479323516441\n",
      "  (0, 3632)\t0.0875816080341\n",
      "  (0, 3917)\t0.175961882925\n",
      "  (0, 3926)\t0.103925922652\n",
      "  (0, 4012)\t0.0726357486844\n",
      "  (0, 4015)\t0.0248885676348\n",
      "  (0, 4183)\t0.0524104347367\n",
      "  (0, 4838)\t0.0439799587509\n",
      "  (0, 5097)\t0.0931229183095\n",
      "  (0, 5129)\t0.0382530477281\n",
      "  (0, 5160)\t0.0457913026157\n",
      "  (0, 5531)\t0.0422200321218\n",
      "  (0, 5611)\t0.065975599349\n",
      "  (0, 5677)\t0.0620086149554\n",
      "  (0, 5849)\t0.0454970644722\n",
      "  (0, 5943)\t0.0600002548772\n",
      "  (0, 6448)\t0.0667051060785\n",
      "  (0, 6754)\t0.0820399484338\n",
      "  :\t:\n",
      "  (0, 66541)\t0.0397347025829\n",
      "  (0, 66602)\t0.0554590041714\n",
      "  (0, 66603)\t0.0524852916919\n",
      "  (0, 67275)\t0.0259308659611\n",
      "  (0, 67457)\t0.0328956273242\n",
      "  (0, 67506)\t0.0289878530289\n",
      "  (0, 68085)\t0.133241693561\n",
      "  (0, 68139)\t0.0644307851994\n",
      "  (0, 68165)\t0.0426360022698\n",
      "  (0, 68516)\t0.0644307851994\n",
      "  (0, 68572)\t0.0846639835942\n",
      "  (0, 68708)\t0.0349797887594\n",
      "  (0, 69868)\t0.0257350804785\n",
      "  (0, 69942)\t0.0875816080341\n",
      "  (0, 71032)\t0.0705225333188\n",
      "  (0, 71282)\t0.0481382945367\n",
      "  (0, 71344)\t0.0479835010988\n",
      "  (0, 71540)\t0.0695256075634\n",
      "  (0, 72688)\t0.0383432197227\n",
      "  (0, 73618)\t0.0416682979322\n",
      "  (0, 73921)\t0.0627023403803\n",
      "  (0, 74166)\t0.0247163440369\n",
      "  (0, 74928)\t0.0200573065903\n",
      "  (0, 74929)\t0.0214899713467\n",
      "  (0, 74930)\t0.0214899713467\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('NE_counts.txt', 'w') as f:\n",
    "#    for i in range(4000):\n",
    "#        f.write(article_keys[i] + ' ' + str(NE_train[i][0]) + ' ' + str(NE_train[i][1]) + ' ' + str(NE_train[i][2]) + '\\n')\n",
    "#    for i in range(len(NE_dev)):\n",
    "#        f.write(article_keys[4000+i] + ' ' + str(NE_dev[i][0]) + ' ' + str(NE_dev[i][1]) + ' ' + str(NE_dev[i][2]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x74928 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1089537 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
