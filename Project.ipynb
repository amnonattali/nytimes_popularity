{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import string\n",
    "import io\n",
    "import re\n",
    "import vocabulary\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_131/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "embedding_file, embedding_dimensions, embedding_cache = \\\n",
    "    \"glove.6B.50d.txt\", 50, \"embedding.npz\"\n",
    "\n",
    "#st = StanfordNERTagger('C:\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz', 'C:\\stanford-ner\\stanford-ner.jar', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('articles_with_texts_final.json') as data_file:\n",
    "    articles = json.load(data_file)\n",
    "article_keys = articles[u'data'].keys()\n",
    "db = {}\n",
    "\n",
    "# with open('1054/mrc2.dct') as f:\n",
    "#     for line in f:\n",
    "#         #line = f.readline()\n",
    "#         match_word = re.search(r'((\\w)*)\\|',line[51:], re.S)\n",
    "#         #print line\n",
    "#         match_img = int(line[31:34]) #:44\n",
    "        \n",
    "#         if match_img != 0:\n",
    "#             db[match_word.group(1)] = match_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_file, vocab_file_type = \"vocab.pkl\", \"pickle\"\n",
    "\n",
    "def iterable() :\n",
    "    for key in article_keys:\n",
    "        tokens = nltk.word_tokenize(articles[u'data'][u'text'])\n",
    "        for o in tokens:\n",
    "            yield o\n",
    "\n",
    "made_vocabulary = False\n",
    "if made_vocabulary :\n",
    "    v = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
    "else: \n",
    "    v = vocabulary.Vocabulary.from_iterable(iterable(),file_type=vocab_file_type)\n",
    "    v.save(vocab_file)\n",
    "v.stop_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_file, embedding_dimensions, embedding_cache = \"glove.6B.50d.txt\", 50, \"embedding.npz\"\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = numpy.load(filename)\n",
    "    return scipy.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape = loader['shape'])\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    numpy.savez(filename, data = array.data, indices=array.indices, indptr =array.indptr, shape=array.shape )\n",
    "    \n",
    "def build_sparse_embedding(vocab, glovefile, d) :\n",
    "    remaining_vocab = vocab.keyset()\n",
    "    embeddings = np.zeros((len(remaining_vocab), d))\n",
    "    \n",
    "    with open(glovefile) as glovedata :\n",
    "        fileiter = glovedata.readlines()\n",
    "        rows = []\n",
    "        columns = []\n",
    "        values = []\n",
    "        \n",
    "        for line in fileiter :\n",
    "            line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            try:\n",
    "                glove_key, nums = line[0], [float(x.strip()) for x in line[1:]]\n",
    "                for word in (glove_key, glove_key.capitalize(), glove_key.upper()) :\n",
    "                    if word in remaining_vocab :\n",
    "                        columns.append(np.arange(len(nums)))\n",
    "                        rows.append(np.full(len(nums), vocab[word]))\n",
    "                        values.append(np.array(nums))\n",
    "                        remaining_vocab.remove(word)\n",
    "            except Exception as e:\n",
    "                print(\"{} broke. exception: {}. line: {}.\".format(word, e, x))\n",
    "\n",
    "        print(\"{} words were not in glove\".format(len(remaining_vocab)))\n",
    "        return scipy.sparse.coo_matrix((np.concatenate(values),\n",
    "                                        (np.concatenate(rows),\n",
    "                                         np.concatenate(columns))),\n",
    "                                        shape=(len(vocab), d)).tocsr()\n",
    "\n",
    "made_embedding = False\n",
    "if made_embedding :\n",
    "    e = load_sparse_csr(embedding_cache)\n",
    "else: \n",
    "    e = build_sparse_embedding(v, embedding_file, embedding_dimensions)\n",
    "    newsreader.save_sparse_csr(embedding_cache, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = \\\n",
    "sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, \n",
    "                                                max_df=0.5, \n",
    "                                                stop_words='english')\n",
    "_ = vectorizer.fit(articles[u'data'][k][u'text'] for k in article_keys[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T_keys = [k for k in article_keys if (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]\n",
    "F_keys = [k for k in article_keys if not (articles[u'data'][k]['most_emailed'] or articles[u'data'][k]['most_shared'] or \\\n",
    "                articles[u'data'][k]['most_viewed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def feature1(c_articles) : #c_articles is [articles[u'data'][k] for k in keys]\n",
    "\n",
    "#author_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "#_ = author_vectorizer.fit(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys])\n",
    "\n",
    "def author(cas):\n",
    "    return author_vectorizer.transform(string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in cas)\n",
    "    \n",
    "# truth with noise\n",
    "def truth(cas):\n",
    "    if len(cas) == 4000 :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[:4000]], (-1, 1))\n",
    "    else :\n",
    "        return numpy.reshape([(1 if random.randint(1,4) > 1 else 0) if k in T_keys else (1 if random.randint(1,4) == 1 else 0) for k in article_keys[4000:]], (-1, 1))\n",
    "\n",
    "def img_rating(cas):\n",
    "    text = [ca[u'text'] for ca in cas]\n",
    "    imagery_val = [sum(db[word] if word in db else 0 for word in article)/(700.0*len(article)+1.0) for article in text]\n",
    "    return (numpy.matrix(imagery_val)).reshape(-1,1)\n",
    "\n",
    "NE_count_file = 'NE_counts.txt'\n",
    "    \n",
    "def NE_count(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    t = st.tag(tokenized_text)\n",
    "    n_persons = 0.0\n",
    "    n_org = 0.0\n",
    "    n_loc = 0.0\n",
    "    if len(t) == 0 :\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    (a, b) = t[0]\n",
    "    if b == u'PERSON' :\n",
    "        n_persons += 1\n",
    "    elif b == u'LOCATION' :\n",
    "        n_loc += 1\n",
    "    elif b == u'ORGANIZATION' :\n",
    "        n_org += 1\n",
    "    prev = b\n",
    "    for (a, b) in t[1:] :\n",
    "        if b != prev :\n",
    "            prev = b\n",
    "            if b == u'PERSON' :\n",
    "                n_persons += 1\n",
    "            elif b == u'LOCATION' :\n",
    "                n_loc += 1\n",
    "            elif b == u'ORGANIZATION' :\n",
    "                n_org += 1\n",
    "    return [n_persons/len(tokenized_text), n_loc/len(tokenized_text), n_org/len(tokenized_text)]\n",
    "\n",
    "def named_entity(keys):\n",
    "    NE_count_dict = {}\n",
    "    if NE_count_file != None :\n",
    "        with open(NE_count_file, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                NE_count_dict[l[0].encode()] = [float(l[1]), float(l[2]), float(l[3])]\n",
    "        return numpy.matrix([NE_count_dict[key] for key in keys])\n",
    "    else:\n",
    "        return numpy.matrix([NE_count(articles[u'data'][key][u'text']) for key in keys])\n",
    "\n",
    "def sparse(cas):\n",
    "    text = [ca[u'text'] for ca in cas]\n",
    "    x = numpy.zeros((len(text), embedding_dimensions))\n",
    "    for i in range(len(text)) :\n",
    "        vec = numpy.zeros(embedding_dimensions)\n",
    "        for token in nltk.word_tokenize(articles[u'data'][u'text']):\n",
    "            if token in v.keyset():\n",
    "                vec = numpy.add(e[v[token]], vec)\n",
    "        x[i] = vec\n",
    "    \n",
    "features = [sparse, author, img_rating]\n",
    "features_k = [named_entity]\n",
    "\n",
    "\n",
    "def getX(key_list):\n",
    "    #x = vectorizer.transform(articles[u'data'][k][u'text'] for k in key_list)\n",
    "    c_articles = [articles[u'data'][k] for k in key_list]\n",
    "    x = sparse(c_articles)\n",
    "    for f in features :\n",
    "        x_new = f(c_articles)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    for f in features_k :\n",
    "        x_new = f(key_list)\n",
    "        x = scipy.sparse.hstack([x, x_new])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[string.replace(string.replace(ca[u'byline'][u'original'], ' ', ''), '.', '') if ca[u'byline'] != [] else \"\" for ca in [articles[u'data'][k] for k in article_keys[:4000]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = getX(article_keys[:4000])\n",
    "y_train = [k in T_keys for k in article_keys[:4000]]\n",
    "X_dev = getX(article_keys[4000:])\n",
    "y_dev = [k in T_keys for k in article_keys[4000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = \\\n",
    "sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                   penalty=\"elasticnet\",\n",
    "                                   n_iter=5)\n",
    "\n",
    "_ = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_dev)\n",
    "print sklearn.metrics.accuracy_score(y_dev, pred)\n",
    "print sklearn.metrics.confusion_matrix(y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word = vectorizer.get_feature_names()\n",
    "best_elts = numpy.argpartition(classifier.coef_, classifier.coef_.size - 20)[0][-20:]\n",
    "best_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in best_elts]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_elts = numpy.argpartition(classifier.coef_, 19)[0][:20]\n",
    "worst_words = [index_to_word[x] if x < len(index_to_word) else x-len(index_to_word) for x in worst_elts]\n",
    "worst_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('NE_counts.txt', 'w') as f:\n",
    "#    for i in range(4000):\n",
    "#        f.write(article_keys[i] + ' ' + str(NE_train[i][0]) + ' ' + str(NE_train[i][1]) + ' ' + str(NE_train[i][2]) + '\\n')\n",
    "#    for i in range(len(NE_dev)):\n",
    "#        f.write(article_keys[4000+i] + ' ' + str(NE_dev[i][0]) + ' ' + str(NE_dev[i][1]) + ' ' + str(NE_dev[i][2]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getX(article_keys[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
